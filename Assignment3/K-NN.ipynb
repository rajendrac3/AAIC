{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in our data (525814, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mounting the dataset from drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive') \n",
    "\n",
    "#connecting to sqlite db\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "                      \n",
    "# filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "# SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000, will give top 500000 data points\n",
    "# you can change the number to any other number based on your computing power\n",
    "\n",
    "# filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000\"\"\", con) \n",
    "# for tsne assignment you can take 5k data points\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3\"\"\", con) \n",
    "\n",
    "# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative\n",
    "print(\"Number of data points in our data\", filtered_data.shape)\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT UserId, ProductId, ProfileName, Time, Score, Text, COUNT(*)\n",
    "FROM Reviews\n",
    "GROUP BY UserId\n",
    "HAVING COUNT(*)>1\n",
    "\"\"\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80668, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R115TNMSPFT9I7</td>\n",
       "      <td>B007Y59HVM</td>\n",
       "      <td>Breyton</td>\n",
       "      <td>1331510400</td>\n",
       "      <td>2</td>\n",
       "      <td>Overall its just OK when considering the price...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife has recurring extreme muscle spasms, u...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R11DNU2NBKQ23Z</td>\n",
       "      <td>B007Y59HVM</td>\n",
       "      <td>Kim Cieszykowski</td>\n",
       "      <td>1348531200</td>\n",
       "      <td>1</td>\n",
       "      <td>This coffee is horrible and unfortunately not ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#oc-R11O5J5ZVQE25C</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>Penguin Chick</td>\n",
       "      <td>1346889600</td>\n",
       "      <td>5</td>\n",
       "      <td>This will be the bottle that you grab from the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#oc-R12KPBODL2B5ZD</td>\n",
       "      <td>B007OSBE1U</td>\n",
       "      <td>Christopher P. Presta</td>\n",
       "      <td>1348617600</td>\n",
       "      <td>1</td>\n",
       "      <td>I didnt like this coffee. Instead of telling y...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId   ProductId             ProfileName        Time  Score  \\\n",
       "0  #oc-R115TNMSPFT9I7  B007Y59HVM                 Breyton  1331510400      2   \n",
       "1  #oc-R11D9D7SHXIJB9  B005HG9ET0  Louis E. Emory \"hoppy\"  1342396800      5   \n",
       "2  #oc-R11DNU2NBKQ23Z  B007Y59HVM        Kim Cieszykowski  1348531200      1   \n",
       "3  #oc-R11O5J5ZVQE25C  B005HG9ET0           Penguin Chick  1346889600      5   \n",
       "4  #oc-R12KPBODL2B5ZD  B007OSBE1U   Christopher P. Presta  1348617600      1   \n",
       "\n",
       "                                                Text  COUNT(*)  \n",
       "0  Overall its just OK when considering the price...         2  \n",
       "1  My wife has recurring extreme muscle spasms, u...         3  \n",
       "2  This coffee is horrible and unfortunately not ...         2  \n",
       "3  This will be the bottle that you grab from the...         3  \n",
       "4  I didnt like this coffee. Instead of telling y...         2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(display.shape)\n",
    "display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364173, 10)\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicate reviews\n",
    "final=filtered_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    307061\n",
       "0     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [00:00<00:00, 447313.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#remove urls from text python\n",
    "from tqdm import tqdm\n",
    "lst = []\n",
    "removed_urls_list = []\n",
    "for text in tqdm(final['Text']):\n",
    "  removed_urls_text = re.sub(r\"http\\S+\", \"\", text)\n",
    "  lst.append(removed_urls_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [00:00<00:00, 452270.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#remove urls from text python\n",
    "removed_urls_list = []\n",
    "for text in tqdm(lst):\n",
    "  removed_urls_text = re.sub(r\"http\\S+\", \"\", text)\n",
    "  removed_urls_list.append(removed_urls_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [01:49<00:00, 3330.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "text_lst = []\n",
    "for text in tqdm(removed_urls_list):\n",
    "  soup = BeautifulSoup(text, 'lxml')\n",
    "  text = soup.get_text()\n",
    "  text_lst.append(text)\n",
    "# print(text)\n",
    "# print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n"
     ]
    }
   ],
   "source": [
    "print(len(final['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [00:05<00:00, 65510.16it/s]\n"
     ]
    }
   ],
   "source": [
    "decat_lst = []\n",
    "for decat_text in tqdm(text_lst):\n",
    "  text = decontracted(decat_text)\n",
    "  decat_lst.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [00:22<00:00, 16465.51it/s]\n"
     ]
    }
   ],
   "source": [
    "strip_list = []\n",
    "for to_strip in tqdm(decat_lst):\n",
    "  text = re.sub(\"\\S*\\d\\S*\", \"\", to_strip).strip()\n",
    "  strip_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [00:12<00:00, 29401.19it/s]\n"
     ]
    }
   ],
   "source": [
    "spatial_list = []\n",
    "for to_spatial in tqdm(strip_list):\n",
    "  text = re.sub('[^A-Za-z0-9]+', ' ', to_spatial)\n",
    "  spatial_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364171/364171 [02:44<00:00, 2216.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(spatial_list):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'satisfied product advertised use cereal raw vinegar general sweetner'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(preprocessed_reviews))\n",
    "preprocessed_reviews[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final['Preprocessed_text'] = preprocessed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>525809</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>great sesame chicken good not better resturant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525810</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>disappointed flavor chocolate notes especially...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525811</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>stars small give one training session tried tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525812</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>best treats training rewarding dog good groomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525813</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>satisfied product advertised use cereal raw vi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId              ProfileName  \\\n",
       "525809  568450  B001EO7N10  A28KG5XORO54AY         Lettie D. Carter   \n",
       "525810  568451  B003S1WTCU  A3I8AFVPEE8KI5                R. Sawyer   \n",
       "525811  568452  B004I613EE  A121AA1GQV751Z            pksd \"pk_007\"   \n",
       "525812  568453  B004I613EE   A3IBEVCTXKNOH  Kathy A. Welch \"katwel\"   \n",
       "525813  568454  B001LR2CU2  A3LGQPJCZVL9UC                 srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "525809                     0                       0      1  1299628800   \n",
       "525810                     0                       0      0  1331251200   \n",
       "525811                     2                       2      1  1329782400   \n",
       "525812                     1                       1      1  1331596800   \n",
       "525813                     0                       0      1  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "525809                 Will not do without   \n",
       "525810                        disappointed   \n",
       "525811            Perfect for our maltipoo   \n",
       "525812  Favorite Training and reward treat   \n",
       "525813                         Great Honey   \n",
       "\n",
       "                                                     Text  \\\n",
       "525809  Great for sesame chicken..this is a good if no...   \n",
       "525810  I'm disappointed with the flavor. The chocolat...   \n",
       "525811  These stars are small, so you can give 10-15 o...   \n",
       "525812  These are the BEST treats for training and rew...   \n",
       "525813  I am very satisfied ,product is as advertised,...   \n",
       "\n",
       "                                        Preprocessed_text  \n",
       "525809  great sesame chicken good not better resturant...  \n",
       "525810  disappointed flavor chocolate notes especially...  \n",
       "525811  stars small give one training session tried tr...  \n",
       "525812  best treats training rewarding dog good groomi...  \n",
       "525813  satisfied product advertised use cereal raw vi...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(final))\n",
    "final.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()\n",
    "conn = sqlite3.connect(os.path.join(dir_path, 'final.sqlite'))\n",
    "# final.to_sql('Reviews', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count(*)\n",
      "0    364171\n"
     ]
    }
   ],
   "source": [
    "review_3 = pd.read_sql_query(\"\"\" SELECT count(*) FROM Reviews\"\"\", conn)\n",
    "print(review_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_data[\"Time\"] = pd.to_datetime(filtered_data[\"Time\"], unit = \"s\")\n",
    "filtered_data = filtered_data.sort_values(by = \"Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117924</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1999-10-08</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>witty little book makes son laugh loud recite ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117901</th>\n",
       "      <td>150501</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>AJ46FKXOVC7NR</td>\n",
       "      <td>Nicholas A Mesiano</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1999-10-25</td>\n",
       "      <td>This whole series is great way to spend time w...</td>\n",
       "      <td>I can remember seeing the show when it aired o...</td>\n",
       "      <td>remember seeing show aired television years ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298792</th>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1999-12-02</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "      <td>beetlejuice well written movie everything exce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169281</th>\n",
       "      <td>230285</td>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1999-12-06</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "      <td>twist rumplestiskin captured film starring mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298791</th>\n",
       "      <td>451855</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AJH6LUC1UT1ON</td>\n",
       "      <td>The Phantom of the Opera</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>FANTASTIC!</td>\n",
       "      <td>Beetlejuice is an excellent and funny movie. K...</td>\n",
       "      <td>beetlejuice excellent funny movie keaton hilar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId               ProfileName  \\\n",
       "117924  150524  0006641040   ACITT7DI6IDDL           shari zychinski   \n",
       "117901  150501  0006641040   AJ46FKXOVC7NR        Nicholas A Mesiano   \n",
       "298792  451856  B00004CXX9   AIUWLEQ1ADEG5          Elizabeth Medina   \n",
       "169281  230285  B00004RYGX  A344SMIA5JECGM           Vincent P. Ross   \n",
       "298791  451855  B00004CXX9   AJH6LUC1UT1ON  The Phantom of the Opera   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score       Time  \\\n",
       "117924                     0                       0      1 1999-10-08   \n",
       "117901                     2                       2      1 1999-10-25   \n",
       "298792                     0                       0      1 1999-12-02   \n",
       "169281                     1                       2      1 1999-12-06   \n",
       "298791                     0                       0      1 2000-01-03   \n",
       "\n",
       "                                                  Summary  \\\n",
       "117924                          EVERY book is educational   \n",
       "117901  This whole series is great way to spend time w...   \n",
       "298792                               Entertainingl Funny!   \n",
       "169281                            A modern day fairy tale   \n",
       "298791                                         FANTASTIC!   \n",
       "\n",
       "                                                     Text  \\\n",
       "117924  this witty little book makes my son laugh at l...   \n",
       "117901  I can remember seeing the show when it aired o...   \n",
       "298792  Beetlejuice is a well written movie ..... ever...   \n",
       "169281  A twist of rumplestiskin captured on film, sta...   \n",
       "298791  Beetlejuice is an excellent and funny movie. K...   \n",
       "\n",
       "                                        Preprocessed_text  \n",
       "117924  witty little book makes son laugh loud recite ...  \n",
       "117901  remember seeing show aired television years ag...  \n",
       "298792  beetlejuice well written movie everything exce...  \n",
       "169281  twist rumplestiskin captured film starring mic...  \n",
       "298791  beetlejuice excellent funny movie keaton hilar...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364171\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 364171 entries, 117924 to 107253\n",
      "Data columns (total 11 columns):\n",
      "Id                        364171 non-null int64\n",
      "ProductId                 364171 non-null object\n",
      "UserId                    364171 non-null object\n",
      "ProfileName               364171 non-null object\n",
      "HelpfulnessNumerator      364171 non-null int64\n",
      "HelpfulnessDenominator    364171 non-null int64\n",
      "Score                     364171 non-null int64\n",
      "Time                      364171 non-null datetime64[ns]\n",
      "Summary                   364171 non-null object\n",
      "Text                      364171 non-null object\n",
      "Preprocessed_text         364171 non-null object\n",
      "dtypes: datetime64[ns](1), int64(4), object(6)\n",
      "memory usage: 33.3+ MB\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_data))\n",
    "filtered_data.info()\n",
    "filtered_data = filtered_data.head(100000)\n",
    "print(len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    87729\n",
       "0    12271\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: 117924    witty little book makes son laugh loud recite ...\n",
      "117901    remember seeing show aired television years ag...\n",
      "298792    beetlejuice well written movie everything exce...\n",
      "169281    twist rumplestiskin captured film starring mic...\n",
      "298791    beetlejuice excellent funny movie keaton hilar...\n",
      "Name: Preprocessed_text, dtype: object\n",
      "None\n",
      "shape of y: 117924    1\n",
      "117901    1\n",
      "298792    1\n",
      "169281    1\n",
      "298791    1\n",
      "Name: Score, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = filtered_data[\"Preprocessed_text\"]\n",
    "print(print(\"shape of X:\", X.head(5)))\n",
    "y = filtered_data[\"Score\"]\n",
    "print(\"shape of y:\", y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,) (60000,) (20000,)\n",
      "(60000,) (60000,) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape)\n",
    "print(X_train.shape, Y_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.1] BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 46047)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_vect = count_vect.fit_transform(X_train)\n",
    "X_test_vect = count_vect.transform(X_test)\n",
    "X_val_vect = count_vect.transform(X_val)\n",
    "BoW_dict = {'X_train_vect':X_train_vect, 'X_test_vect': X_test_vect, 'X_val_vect': X_val_vect}\n",
    "print(X_train_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('BoW.pkl', 'wb') as handle:\n",
    "    pickle.dump(BoW_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# [4.3] TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of out text TFIDF vectorizer  (60000, 33754)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "train_tf_idf = tf_idf_vect.fit_transform(X_train)\n",
    "cv_tf_idf = tf_idf_vect.transform(X_val)\n",
    "test_tf_idf = tf_idf_vect.transform(X_test)\n",
    "\n",
    "print(\"the shape of out text TFIDF vectorizer \",train_tf_idf.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf_dict = {'train_tf_idf': train_tf_idf, 'cv_tf_idf': cv_tf_idf, 'test_tf_idf': test_tf_idf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tf_idf.pkl', 'wb') as handle:\n",
    "    pickle.dump(tf_idf_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.4] Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sentance=[]\n",
    "for sentance in X:\n",
    "    list_of_sentance.append(sentance.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrific', 0.8509088158607483), ('fantastic', 0.8405745029449463), ('good', 0.8284103870391846), ('awesome', 0.8274010419845581), ('excellent', 0.822074830532074), ('wonderful', 0.8163369297981262), ('perfect', 0.7458590865135193), ('incredible', 0.7360571026802063), ('amazing', 0.7160287499427795), ('nice', 0.7077826261520386)]\n",
      "==================================================\n",
      "[('greatest', 0.7847960591316223), ('best', 0.720042884349823), ('tastiest', 0.6726945638656616), ('nastiest', 0.6436268091201782), ('nicest', 0.6161113381385803), ('disgusting', 0.6049458384513855), ('worse', 0.5945631265640259), ('wins', 0.5913236141204834), ('superior', 0.5855969190597534), ('inedible', 0.5740874409675598)]\n"
     ]
    }
   ],
   "source": [
    "is_your_ram_gt_16g=False\n",
    "want_to_use_google_w2v = False\n",
    "want_to_train_w2v = True\n",
    "\n",
    "if want_to_train_w2v:\n",
    "    # min_count = 5 considers only words that occured atleast 5 times\n",
    "    w2v_model=Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\n",
    "    print(w2v_model.wv.most_similar('great'))\n",
    "    print('='*50)\n",
    "    print(w2v_model.wv.most_similar('worst'))\n",
    "    \n",
    "elif want_to_use_google_w2v and is_your_ram_gt_16g:\n",
    "    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n",
    "        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "        print(w2v_model.wv.most_similar('great'))\n",
    "        print(w2v_model.wv.most_similar('worst'))\n",
    "    else:\n",
    "        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  18823\n",
      "sample words  ['iceberg', 'convenient', 'sorbate', 'arrangement', 'oder', 'skunk', 'momma', 'phenolic', 'rarified', 'indicative', 'glee', 'liquified', 'plagued', 'atlantic', 'tickling', 'ease', 'peaceful', 'crumpets', 'experation', 'ths', 'flipped', 'confiscated', 'queso', 'regained', 'counteract', 'radical', 'moisturizes', 'honeydew', 'jug', 'becaue', 'triclopyr', 'torte', 'carefully', 'lowers', 'flay', 'cumin', 'plantations', 'jams', 'definition', 'fluff', 'vice', 'organix', 'grained', 'guar', 'gramma', 'avocados', 'bottarga', 'shelling', 'buffalo', 'rediculous']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.4.1] Converting text into vectors using Avg W2V, TFIDF-W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.4.1.1] Avg W2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [27:50<00:00, 59.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_avgw2v, X_test_avgw2v, Y_train_avgw2v, Y_test_avgw2v = train_test_split(sent_vectors, y, test_size=0.2, random_state = 42)\n",
    "X_train_avgw2v, X_val_avgw2v, Y_train_avgw2v, Y_val_avgw2v = train_test_split(X_train_avgw2v, Y_train_avgw2v, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Avg_w2v_dict = {'X_train_avgw2v':X_train_avgw2v, 'Y_train_avgw2v': Y_train_avgw2v,  \n",
    "                     'X_val_avgw2v': X_val_avgw2v, 'Y_val_avgw2v': Y_val_avgw2v,\n",
    "                    'X_test_avgw2v': X_test_avgw2v, 'Y_test_avgw2v': Y_test_avgw2v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('avg_w2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(Avg_w2v_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4.4.1.2] TFIDF weighted W2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "model = TfidfVectorizer()\n",
    "tf_idf_matrix = model.fit_transform(X_train)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [2:04:10<00:00, 13.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in tqdm(list_of_sentance): # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vec = w2v_model.wv[word]\n",
    "#             tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n",
    "            # to reduce the computation we are \n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidfw2v, X_test_tfidfw2v, Y_train_tfidfw2v, Y_test_tfidfw2v = train_test_split(sent_vectors, y, test_size=0.2, random_state = 42)\n",
    "X_train_tfidfw2v, X_val_tfidfw2v, Y_train_tfidfw2v, Y_val_tfidfw2v = train_test_split(X_train_tfidfw2v, Y_train_tfidfw2v, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_w2v_dict = {'X_train_tfidfw2v':X_train_tfidfw2v, 'Y_train_tfidfw2v': Y_train_tfidfw2v,  \n",
    "                     'X_val_tfidfw2v': X_val_tfidfw2v, 'Y_val_tfidfw2v': Y_val_tfidfw2v,\n",
    "                    'X_test_tfidfw2v': X_test_tfidfw2v, 'Y_test_tfidfw2v': Y_test_tfidfw2v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tfidf_w2v.pkl', 'wb') as handle:\n",
    "    pickle.dump(tfidf_w2v_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K - NN on Bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r\"BoW.pkl\", \"rb\") as input_file:\n",
    "    BoW_dict = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "k_acc = []\n",
    "k_list = []\n",
    "auc_train = []\n",
    "auc_cv = []\n",
    "auc_test = []\n",
    "fpr_train = dict()\n",
    "tpr_train = dict()\n",
    "fpr_test = dict()\n",
    "tpr_test = dict()\n",
    "fpr_cv = dict()\n",
    "tpr_cv = dict()\n",
    "\n",
    "for k_value in tqdm(range(1, 30, 2)):\n",
    "    knn = KNeighborsClassifier(n_neighbors = k_value, algorithm='brute')\n",
    "    knn.fit(BoW_dict['X_train_vect'],Y_train)\n",
    "    train_proba = knn.predict_proba(BoW_dict['X_train_vect'])\n",
    "    fpr_train[k], tpr_train[k], _ = roc_curve(Y_train, train_proba[:,1])\n",
    "    auc_train.append(auc(fpr_train[k], tpr_train[k]))\n",
    "        \n",
    "    cv_proba = knn.predict_proba(BoW_dict['X_val_vect'])\n",
    "    fpr_cv[k], tpr_cv[k], _ = roc_curve(Y_val_vect, cv_proba[:,1])\n",
    "    auc_cv.append(auc(fpr_cv[k], tpr_cv[k]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
